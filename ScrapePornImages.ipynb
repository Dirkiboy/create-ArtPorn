{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Scrape Porn Images üêôüì∑"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This Notebook scrapes the images to train the `Discriminator` of the **CreateArtPorn Neural Network** from [pornpics.com](https://www.pornpics.com). You can also download pictures manually by:  \n",
    "1. chosing a category on [www.pornpics.com](https://www.pornpics.com),   \n",
    "2. scrolling down the page (you need to continue scrolling down as pictures show up \"on the fly\",   \n",
    "3. saving website locally (this gives you a among all other files a folder with all images).\n",
    "\n",
    "Anyhow, as websites html cang change this code might be out of date when you use it. You can test if by running the cell `Code still working?`. If it does not throw any error, chances are high you can use tis notebook:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import shutil #to save locally\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Constants"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "IMAGE_FOLDER = \"input/porn-pics\" #<- download parent folder fo\n",
    "CHROMEDRIVER_PATH = \"/Applications/chromedriver\" #<- get Chromedriver here: https://chromedriver.chromium.org/downloads\n",
    "URL = \"https://www.pornpics.com\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code still working?\n",
    "> Rund this cell to check if the website hasn't changed yet. If everything is fine it should show 2 pictures. **It'll take ca. 1 minute and needs Chromedriver to be installed!**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# CODE IS WORKING?\n",
    "from matplotlib import pyplot as plt\n",
    "driver = webdriver.Chrome(CHROMEDRIVER_PATH)\n",
    "driver.get(URL)\n",
    "time.sleep(10)\n",
    "driver.execute_script(\"window.scrollTo(1, document.body.scrollHeight);\")\n",
    "time.sleep(5)\n",
    "soup = bs(driver.page_source, 'html.parser')\n",
    "driver.close()\n",
    "main = soup.find_all(\"div\", id=\"main\")\n",
    "imgs = main[0].find_all(\"img\")\n",
    "urls = [img[\"data-src\"] for img in imgs]\n",
    "imgs = random.sample(urls, k=3)\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "i=0\n",
    "for axe in axes:\n",
    "    a = plt.imread(imgs[i],format=\"jpg\")\n",
    "    i+=1\n",
    "    axe.imshow(a)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scrape Categories and Tags\n",
    ">On the first page are porn genre categories together with links to their main pages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Categories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# SCRAPE CATEGORIES\n",
    "#\n",
    "# # extract categories and urls\n",
    "soup = bs(requests.get(URL).content, \"html.parser\")\n",
    "elements = soup.find_all(\"a\", {\"class\": \"rel-link\"})\n",
    "for el in elements:\n",
    "    category = el[\"href\"][1:-1]\n",
    "    cat_list = [el[\"href\"][1:-1] for el in elements]\n",
    "    \n",
    "link_list = [\"https://www.pornpics.com/\"+cat for cat in cat_list]\n",
    "\n",
    "# Put tags and links in DataFrame\n",
    "category_df = pd.DataFrame(columns=[\"category\", \"url\"], data= {\"category\": cat_list, \"url\":link_list})\n",
    "    #print(category)\n",
    "\n",
    "# Show results\n",
    "print(\"Rows: \", len(category_df))\n",
    "category_df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rows:  155\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>japanese</td>\n",
       "      <td>https://www.pornpics.com/japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asian</td>\n",
       "      <td>https://www.pornpics.com/asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>european</td>\n",
       "      <td>https://www.pornpics.com/european</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>teen</td>\n",
       "      <td>https://www.pornpics.com/teen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pussy</td>\n",
       "      <td>https://www.pornpics.com/pussy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                url\n",
       "0  japanese  https://www.pornpics.com/japanese\n",
       "1     asian     https://www.pornpics.com/asian\n",
       "2  european  https://www.pornpics.com/european\n",
       "3      teen      https://www.pornpics.com/teen\n",
       "4     pussy     https://www.pornpics.com/pussy"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tags"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "url = URL+\"/tags/\"\n",
    "soup = bs(requests.get(url).content, \"html.parser\")\n",
    "\n",
    "# Scrape tags and links to tag pages\n",
    "elements = soup.find(\"div\", id= \"main-list\").find_all(\"a\")\n",
    "for el in elements:\n",
    "    tag_list = [el[\"href\"][6:-1] for el in elements if el[\"href\"].startswith(\"/tags\")]\n",
    "link_list = [URL+tag for tag in tag_list]\n",
    "\n",
    "# Put tags and links in DataFrame\n",
    "tags_df = pd.DataFrame(columns=[\"tags\", \"url\"], data= {\"tags\": tag_list, \"url\":link_list})\n",
    "\n",
    "# Show results\n",
    "print(\"Rows: \", len(tags_df))\n",
    "tags_df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rows:  1526\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amateur-anal</td>\n",
       "      <td>https://www.pornpics.comamateur-anal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amateur-ass</td>\n",
       "      <td>https://www.pornpics.comamateur-ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amateur-beach</td>\n",
       "      <td>https://www.pornpics.comamateur-beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amateur-bikini</td>\n",
       "      <td>https://www.pornpics.comamateur-bikini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amateur-blowjob</td>\n",
       "      <td>https://www.pornpics.comamateur-blowjob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tags                                      url\n",
       "0     amateur-anal     https://www.pornpics.comamateur-anal\n",
       "1      amateur-ass      https://www.pornpics.comamateur-ass\n",
       "2    amateur-beach    https://www.pornpics.comamateur-beach\n",
       "3   amateur-bikini   https://www.pornpics.comamateur-bikini\n",
       "4  amateur-blowjob  https://www.pornpics.comamateur-blowjob"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scrape Images\n",
    ">The following cell starts the functions for the Web Crawler. The cell after starts it. In the latter you can decide for which tags and categories the scrawler should download images. The dataframes `tags_categories` and `tags_df` shows you possible values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# FUNCTIONS TO SCRAPE & DOWNLOAD IMAGES FROM CATEGORY AND/OR TAG\n",
    "\n",
    "def get_soup(url):\n",
    "    '''\n",
    "    Extract html text from a webside where user needs to scroll down to see whole content\n",
    "    '''\n",
    "    driver = webdriver.Chrome(CHROMEDRIVER_PATH)\n",
    "    driver.get(url)\n",
    "    time.sleep(10)\n",
    "    this = True\n",
    "    last = False\n",
    "    n=0\n",
    "    while this != last:\n",
    "        last = this\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        this = driver.page_source\n",
    "        n+=1\n",
    "        \n",
    "    soup = bs(this, 'html.parser')\n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def scrape_images(soup, save_path, df, df_failed, category=None, tag=None, seperate_folders=True):\n",
    "    '''\n",
    "    Web Crawler to extract all porn images within each category or tag website. \n",
    "    '''\n",
    "    t0=time.time() #<-start timer\n",
    "\n",
    "    #scrape image\n",
    "    main = soup.find_all(\"div\", id=\"main\")\n",
    "    imgs = main[0].find_all(\"img\")\n",
    "    for img in imgs:\n",
    "        d = img[\"alt\"] #<-description of image\n",
    "        u = img[\"data-src\"] #<-url of image\n",
    "        n = Path(u).name #<-extract file name\n",
    "        try:\n",
    "            urllib.request.urlretrieve(u, os.path.join(save_path, n)) #<-download image\n",
    "            df = df.append({\"name\":n, \"url\":u, \"description\":d, \"category\":category, \"tags\":tag}, ignore_index=True)\n",
    "            \n",
    "        except: #<-document failed downloads\n",
    "            df_failed = df_failed.append({\"name\":n, \"url\":u, \"description\":d}, ignore_index=True)\n",
    "\n",
    "    t1=time.time() #<-stop timer\n",
    "    print(\n",
    "        \"downloaded {} images out of {} extracted image-urls in {} min and saved in {}\".format(\n",
    "            len(df), len(df)+len(df_failed), int((t1-t0)/60), save_path )\n",
    "            )\n",
    "    return df, df_failed\n",
    "\n",
    "def download_images(categories=None, tags=None):\n",
    "    t0=time.time() #<-start timer\n",
    "    #create dataframes for storing\n",
    "    df = pd.DataFrame(columns=[\"name\", \"url\", \"description\", \"category\", \"tags\"])\n",
    "    df_failed = pd.DataFrame(columns=[\"name\", \"url\", \"description\"])\n",
    "    \n",
    "    # scrape categories\n",
    "    if categories:\n",
    "        for cat in categories:\n",
    "            url = \"https://www.pornpics.com/\"+cat+\"/\"         \n",
    "            try:\n",
    "                soup = get_soup(url) #<- run get_soup() üí´\n",
    "                \n",
    "                # create download folder\n",
    "                subfolder = \"category_\"+cat\n",
    "                dir = os.path.join(IMAGE_FOLDER, subfolder)\n",
    "                \n",
    "                if not os.path.exists(dir):\n",
    "                    os.makedirs(dir)\n",
    "                try:\n",
    "                    df, df_failed = scrape_images(soup, dir, df, df_failed, category=cat) #<- run scrape_images() üí´          \n",
    "                except:\n",
    "                    print(\"could not scrape {} from category: {}\".format(cat, url))\n",
    "                    pass\n",
    "            except:\n",
    "                print(\"Could not scrape category: \", cat)\n",
    "\n",
    "    if tags:\n",
    "        cat=None\n",
    "        for tag in tags:\n",
    "            url = \"https://www.pornpics.com/tags/\"+tag+\"/\"\n",
    "            try:\n",
    "                soup = get_soup(url) #<- run get_soup() üí´\n",
    "\n",
    "                # create download folder\n",
    "                subfolder = \"tag_\"+tag\n",
    "                dir = os.path.join(IMAGE_FOLDER, subfolder)\n",
    "                if not os.path.exists(dir):\n",
    "                    os.makedirs(dir)\n",
    "                try:\n",
    "                    df, df_failed = scrape_images(soup, dir, df, df_failed, tag=tag) #<- run def scrape_images()         \n",
    "                except:\n",
    "                    print(\"could not scrape {} from tag: {}\".format(tag, url))\n",
    "                    pass\n",
    "            except:\n",
    "                    print(\"Could not scrape tag: \", tag)\n",
    "\n",
    "        t1=time.time() #<-stop timer\n",
    "        print(\"Total scraping time: {} min\".format( int((t1-t0)/60) ) )\n",
    "    return df, df_failed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üëáüèª üöÄüß® Chose Categories and/or Tags for downloading and start Crawler üêô"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Chose which categories and tags should be downloaded\n",
    "\n",
    "# # Select categories and tags by search pattern:\n",
    "category = category_df[category_df.category.str.contains('blowjob', regex=False)].category.values.tolist()\n",
    "tags = tags_df[tags_df.tags.str.contains('blowjob', regex=False)].tags.values.tolist()\n",
    "\n",
    "# or add manually:\n",
    "categories = [\"blowjob\"]\n",
    "tags = [\"cum-on-face\", \"pov-blowjob\"]\n",
    "\n",
    "# RUN CODE (! ca 10min per category, tag)\n",
    "# ! will open Chrome in the background !\n",
    "df, df_failed = download_images(categories,tags)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Category blowjob not in <meta_df>!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Rows: \", len(df_failed))\n",
    "df_failed"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rows:  2\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49109844</td>\n",
       "      <td>https://img.strpst.com/us4/previews/1638951709...</td>\n",
       "      <td>Wet__Bunny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15739582</td>\n",
       "      <td>https://img.strpst.com/us19/previews/163895185...</td>\n",
       "      <td>madonnaaaa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                                url description\n",
       "0  49109844  https://img.strpst.com/us4/previews/1638951709...  Wet__Bunny\n",
       "1  15739582  https://img.strpst.com/us19/previews/163895185...  madonnaaaa"
      ]
     },
     "metadata": {},
     "execution_count": 859
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Rows: \", len(df))\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rows:  1000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37832869_018_bbed.jpg</td>\n",
       "      <td>https://cdni.pornpics.com/460/1/170/37832869/3...</td>\n",
       "      <td>Petite girl Kimmy Granger takes off sexy linge...</td>\n",
       "      <td>None</td>\n",
       "      <td>pov-blowjob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13503756_004_6fdc.jpg</td>\n",
       "      <td>https://cdni.pornpics.com/460/5/250/13503756/1...</td>\n",
       "      <td>Pretty girl Adriana Chechik nearly deepthroats...</td>\n",
       "      <td>None</td>\n",
       "      <td>pov-blowjob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98825470_006_4f50.jpg</td>\n",
       "      <td>https://cdni.pornpics.com/460/1/337/98825470/9...</td>\n",
       "      <td>Ebony chick sports braided blonde hair during ...</td>\n",
       "      <td>None</td>\n",
       "      <td>pov-blowjob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12710866_009_9c75.jpg</td>\n",
       "      <td>https://cdni.pornpics.com/460/1/123/12710866/1...</td>\n",
       "      <td>Skinny Janice Griffith wraps her big mouth aro...</td>\n",
       "      <td>None</td>\n",
       "      <td>pov-blowjob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76914766_001_6587.jpg</td>\n",
       "      <td>https://cdni.pornpics.com/460/5/177/76914766/7...</td>\n",
       "      <td>POV cock sucking and cum swallowing by Summer ...</td>\n",
       "      <td>None</td>\n",
       "      <td>pov-blowjob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                                url  \\\n",
       "0  37832869_018_bbed.jpg  https://cdni.pornpics.com/460/1/170/37832869/3...   \n",
       "1  13503756_004_6fdc.jpg  https://cdni.pornpics.com/460/5/250/13503756/1...   \n",
       "2  98825470_006_4f50.jpg  https://cdni.pornpics.com/460/1/337/98825470/9...   \n",
       "3  12710866_009_9c75.jpg  https://cdni.pornpics.com/460/1/123/12710866/1...   \n",
       "4  76914766_001_6587.jpg  https://cdni.pornpics.com/460/5/177/76914766/7...   \n",
       "\n",
       "                                         description category         tags  \n",
       "0  Petite girl Kimmy Granger takes off sexy linge...     None  pov-blowjob  \n",
       "1  Pretty girl Adriana Chechik nearly deepthroats...     None  pov-blowjob  \n",
       "2  Ebony chick sports braided blonde hair during ...     None  pov-blowjob  \n",
       "3  Skinny Janice Griffith wraps her big mouth aro...     None  pov-blowjob  \n",
       "4  POV cock sucking and cum swallowing by Summer ...     None  pov-blowjob  "
      ]
     },
     "metadata": {},
     "execution_count": 860
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(df.groupby('category').count().name)\n",
    "print(df.groupby('tags').count().name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "category\n",
      "blowjob    1000\n",
      "Name: name, dtype: int64\n",
      "tags\n",
      "cum-on-face    1000\n",
      "pov-blowjob    1000\n",
      "Name: name, dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking downloaded image files (Optional)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ‚ùóÔ∏è this function needs PIL-library\n",
    "def check_images(dir, endings=[\"jpg\",\"png\"]):\n",
    "    from PIL import Image\n",
    "    results=[]\n",
    "    def loop(ending):\n",
    "        for filename in os.listdir(dir):\n",
    "            if filename.endswith('.'+ending):\n",
    "                try:\n",
    "                    img = Image.open(os.path.join(dir,filename)) # open the image file\n",
    "                    img.verify() # verify that it is, in fact an image\n",
    "                except (IOError, SyntaxError) as e:\n",
    "                    #print('Bad file:', filename) # print out the names of corrupt files\n",
    "                    results.append(filename)\n",
    "    for ending in endings:\n",
    "        loop(ending)\n",
    "\n",
    "    print(\"Found {} broken images\".format(len(results)))\n",
    "    return results\n",
    "\n",
    "\n",
    "for dir in os.listdir(IMAGE_FOLDER):\n",
    "    dir = os.path.join(IMAGE_FOLDER,dir)\n",
    "    check_images(dir)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 0 broken images\n",
      "Found 0 broken images\n",
      "Found 0 broken images\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "- [Medium: How to Download an Image Using Python](https://towardsdatascience.com/how-to-download-an-image-using-python-38a75cfa21c)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Code Friedhof üè¥‚Äç‚ò†Ô∏è\n",
    "\n",
    "> NOTHING TO SEE HERE! I just use these section for some code fragments I want to keep - at least for the moment. The `%%script echo skipping` line stops them from executing when pressing `Run All`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%script echo skipping\n",
    "def download(url, pathname=\"pics\", tid):\n",
    "    \"\"\"\n",
    "    Downloads a file given an URL and puts it in the folder `pathname`\n",
    "    \"\"\"\n",
    "    # if path doesn't exist, make that path dir\n",
    "    if not os.path.isdir(pathname):\n",
    "        os.makedirs(pathname)\n",
    "    # download the body of response by chunk, not immediately\n",
    "    response = requests.get(url, stream=True)\n",
    "    # get the total file size\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "    # get the file name\n",
    "    filename = os.path.join(pathname, tid)\n",
    "    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "    progress = tqdm(response.iter_content(1024), f\"Downloading {filename}\", total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for data in progress:\n",
    "            # write data read to the file\n",
    "            f.write(data)\n",
    "            # update the progress bar manually\n",
    "            progress.update(len(data))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%script echo skipping\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "listImages = os.listdir(IMAGE_FOLDER)\n",
    "\n",
    "for img in listImages:\n",
    "    if img.startswith(\".\"):\n",
    "        pass\n",
    "    else:\n",
    "        imgPath = os.path.join(imageFolder,img)\n",
    "                \n",
    "        try:\n",
    "            img = Image.open(imgPath)\n",
    "            exif_data = img._getexif()\n",
    "        except ValueError as err:\n",
    "            print(err)\n",
    "            print(\"Error on image: \", img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%script echo skipping\n",
    "\n",
    "# SCRAPE TAGS\n",
    "# # create DataFrame for storing categories\n",
    "cat_df = pd.DataFrame(columns=[\"category\", \"url\", \"tags\"])\n",
    "\n",
    "# extract categories and urls\n",
    "soup = bs(requests.get(URL).content, \"html.parser\")\n",
    "tags = soup.find_all(\"a\", {\"class\": \"rel-link\"})\n",
    "for tag in tags:\n",
    "    category = tag[\"href\"][1:-1]\n",
    "    src = \"https://www.pornpics.com/\"+ category +\"/\"\n",
    "    cat_df = cat_df.append({\"category\":category, \"url\":src}, ignore_index=True)\n",
    "\n",
    "cat_df.tags=[set()]*len(cat_df) #<-convert column tags to set() to store multiple values\n",
    "cat_df = cat_df.set_index(\"category\")\n",
    "# check result\n",
    "print(\"Number of categories: \", len(cat_df))\n",
    "cat_df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of categories:  155\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>japanese</th>\n",
       "      <td>https://www.pornpics.com/japanese/</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>https://www.pornpics.com/asian/</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>european</th>\n",
       "      <td>https://www.pornpics.com/european/</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teen</th>\n",
       "      <td>https://www.pornpics.com/teen/</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pussy</th>\n",
       "      <td>https://www.pornpics.com/pussy/</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         url tags\n",
       "category                                         \n",
       "japanese  https://www.pornpics.com/japanese/   {}\n",
       "asian        https://www.pornpics.com/asian/   {}\n",
       "european  https://www.pornpics.com/european/   {}\n",
       "teen          https://www.pornpics.com/teen/   {}\n",
       "pussy        https://www.pornpics.com/pussy/   {}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%script echo skipping\n",
    "# SCRAPE TAGS\n",
    "t0=time.time()\n",
    "not_scraped={}\n",
    "tag_dict = {}\n",
    "for index, row in meta_df.iterrows(): \n",
    "#for index, row in meta_df.iterrows(): \n",
    "    url = row[\"url\"]\n",
    "    cat = index\n",
    "    try:\n",
    "        soup = bs(requests.get(url).content, \"html.parser\")\n",
    "        ul = soup.find_all(\"ul\", \"clearfix\")[0]\n",
    "        ul = ul.find_all(\"li\")\n",
    "        for li in ul:\n",
    "            tag = li.find(\"a\")[\"href\"]\n",
    "            if \"tags\" in tag: #some \"tags\" are categories\n",
    "                tag=tag[6:-1]\n",
    "                if cat not in tag_dict:\n",
    "                    tag_dict[cat] = set()\n",
    "                tag_dict[cat].add(tag)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "    except:\n",
    "        not_scraped[tag] = url\n",
    "        print(\"could not scrape category {} on {}\".format(cat,url))\n",
    "\n",
    "t1=time.time()\n",
    "print(\"Scraping Time: {} min\".format(round((t1-t0)/60),1) )\n",
    "\n",
    "# put scraped tags into meta_df\n",
    "for k,v in tag_dict.items():\n",
    "    meta_df.loc[k, \"tags\"] = v"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "could not scrape category anal-gape on https://www.pornpics.com/anal-gape/\n",
      "Scraping Time: 2 min\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('DataScience': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "ebd19621c3c5e2168748c8eeb47d74470c6c62cc290442347b68e602537f054d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}